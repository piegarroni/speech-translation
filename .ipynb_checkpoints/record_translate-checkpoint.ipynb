{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2fe73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.io.wavfile import write, read\n",
    "import whisper\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "print(sd.query_devices(device=None, kind=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def record():\n",
    "    \"\"\"\n",
    "    Method to record the audio signal from the computer's microphone\n",
    "    \"\"\"\n",
    "    Fs = 8000  # Sampling frequency\n",
    "    duration = 7 # Recording duration in seconds\n",
    "    recording = sd.rec(frames=duration * Fs, samplerate=Fs, channels=1)  # Capture the voice\n",
    "    print('start')\n",
    "    sd.wait()\n",
    "    print('end')\n",
    "\n",
    "    # save file\n",
    "    filename = str(os.getcwd()) + \"\\\\speech-translation\\\\recording\\\\audio.wav\"\n",
    "    write(filename= filename, rate = Fs, data=recording)\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_audio(filename):\n",
    "    \"\"\"\n",
    "    Method to plot the audio signal recorded \n",
    "    \"\"\"\n",
    "    a = read(filename)\n",
    "    recording = np.array(a[1],dtype=float)\n",
    "    # plot audio file\n",
    "    time = np.linspace(0, len(recording - 1) / 8000, len(recording - 1))\n",
    "    print(recording)  \n",
    "    plt.plot(time, recording)  \n",
    "    plt.title(\"Voice Signal\")\n",
    "    plt.xlabel(\"Time [seconds]\")\n",
    "    plt.ylabel(\"Voice amplitude\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4846fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def transcribe(filename):\n",
    "    #filename = filename.split(\"\\\\\")[-2] + \"\\\\\" + filename.split(\"\\\\\")[-1]\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(filename)\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0099155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(filename):\n",
    "    \"\"\"\n",
    "    Translate from french (french speech to english transcrption)\n",
    "    \"\"\"\n",
    "\n",
    "    # load model and processor\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n",
    "\n",
    "    # load dummy dataset and read soundfiles\n",
    "    #ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n",
    "    #ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "    #input_speech = next(iter(ds))[\"audio\"][\"array\"]\n",
    "\n",
    "    # tokenize\n",
    "    input_features = processor(filename, return_tensors=\"pt\").input_features \n",
    "    forced_decoder_ids = processor.get_decoder_prompt_ids(language = \"fr\", task = \"translate\")\n",
    "\n",
    "    predicted_ids = model.generate(input_features, forced_decoder_ids = forced_decoder_ids)\n",
    "    translation = processor.batch_decode(predicted_ids, skip_special_tokens = True)\n",
    "    print(translation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2187edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_language(filename):\n",
    "    \"\"\"\n",
    "    detect language to then pass variable to translate()\n",
    "    \"\"\"\n",
    "\n",
    "    model = whisper.load_model(\"base\")\n",
    "\n",
    "    # load audio and pad/trim it to fit 30 seconds\n",
    "    audio = whisper.load_audio(filename)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "    # make log-Mel spectrogram and move to the same device as the model\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "    # detect the spoken language\n",
    "    _, probs = model.detect_language(mel)\n",
    "    language = {max(probs, key=probs.get)}\n",
    "\n",
    "    # decode the audio\n",
    "    options = whisper.DecodingOptions(fp16 = False)\n",
    "    result = whisper.decode(model, mel, options)\n",
    "\n",
    "    # print the recognized text\n",
    "    print(result.text)\n",
    "    return language\n",
    "\n",
    "\n",
    "file = record()\n",
    "transcription = transcribe(file)\n",
    "print(\"transcription: \" + transcription)\n",
    "\n",
    "\n",
    "print()\n",
    "#print('translation:' + translate(file))\n",
    "print()\n",
    "\n",
    "language = detect_language(file)\n",
    "print(\"Detected language: \" + str(language))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
